# Transformer-Based Teachinng Assistant
This was my big final project for my Computer Science undergraduates course, which was, from scratch, pretraining and fine-tuning a transformer-based model to give a mark and feedback to an input which consisted of:
- question - e.g. "What is 2 + 2"
- marking criteria - e.g. "If student answers 4, give full marks"
- Total marks - e.g. "1"
The corresponding expected output of the fine-tuned transformer-based model should have consisted of: 
- marks - e.g. "1" if correct or "0" if incorrect
- feedback - e.g. "Your answer is incorrect try again, this time, ensure you have added correctly."

For more details on the entire project click this link to access the paper: https://drive.google.com/file/d/13F2R1bziUjhzPMcqMSaj0OMmZSi5X_Sl/view?usp=drive_link

However, The model did not perform well due to this being my fist time building a transformer model, the lack of technical knowledge of array/tensor manipulation, dimensions in arrays/tensors, etc, lead the model not to perform well and output poorly crafted reponses when prompted. Furthermore if one inspects the code for the transformer model will realize that there are prevlant bugs that need to be fixed that can potentially fix the model's performance. This will further be explained further down this readme page.

# Features
- Transformer-based model that can mark and give feedback to student answers according to the marking criteria given
- Transformer was pretrained and fine-tuned from scratch using **Pytorch**
- Encoder-Decoder architecture
- model can be prompted Via the prompting.py file

# Datasets

### Pretraining
For pretraining, the dataset that was used is the 'openwebtext' for more informtation on this dataset click this link: https://skylion007.github.io/OpenWebTextCorpus/

### Finetuning
The fine-tuning dataset is a Json file that is strcutured as:
```json
{
    "question": "question..............",
    "Marking Criteria": "1 mark for each pair of correctly identified labels (rounded down). Each label used only once. Incorrect or repeated usage of labels results in no marks for those labels.",
    "Total marks": 4,
    "answers": [
        {
            "type": "correct",
            "answer": "Corect Answer.............",
            "marks": 4,
            "feedback": "feedback.........."
        },
        {
            "type": "partial",
            "answer": "Partially correct answer.....",
            "marks": 2,
            "feedback": "feedback.........."
        },
        {
            "type": "incorrect",
            "answer": "Incorrect Answer.........",
            "marks": 0,
            "feedback": "Feedback........"
        }
    ]
}
```
The fields: Question, Marking Criteria and Total marks, are filled with data derived from public exam papers from various exam boards (OCR, Edexcel, etc). However, the student answers and feedback to these questions were synthetically generated by the GPT 3.5 model.

The Fine-tuning dataset consists of 470 questions each with thier respective marking criteria, marks, answers and feedback. 
### Test data
the test dataset is the same as the fine tuning dataset but contains 9 new questions that the model did not train on.
# Installation 
## Clone the repository 
```bash
git clone https://github.com/H1548/LLM-Teaching-Assistant.git
cd LLM-Teaching-Assistant
```
## Install Dependencies
```bash 
pip install -r requirements.txt
```
## pretrain transformer 
```bash 
python pretraintrainingloop.py
```
## Fine-tune the transformer 
load the correct checkpoint within the 'finetuningtrainloop.py' file before running the finetuning trainingloop file
```bash
python finetuningtrainloop.py
```
## evaluate model
once again after fine tuning, checkpoints should be stored in the 'fine-tuningcheckpoint' folder, load the correct checkpoint within the source code file 'Evaluate_LLM.py'
```bash 
python Evaluate_LLM.py
```
## Prompt the model
once again load the correct checkpoints, and run the prompting.py file, you will be met with a 'Prompt:', here you need to provide an input which consists of:
- The exam question
- the corresponding answer
- the relevant marking criteria 
- the total marks of the question
```bash 
python Prompting.py
Prompt: Prompt goes here....
```
# Results
As this was my first time evaluating the transformers ability to generate text, the execution of these tests may have not been executed well.

- Train-loss: 0.0482
- Val-loss: 0.0486
- Accuracy score: 0.0048
- BLEU Score: 3.6615
- METEOR Score: 0.001156

As shown in those results despite the really low train and val loss during fine-tuning the model performed very poor when its text generation was evaluated with the use of metrics like accuracy, BLEU Score and the METEOR Score.

# Project Structure
```text
.
|-- DataSet/      # folder stores datasets that are used for training
|-- EncDecCheckpoint   # folder stores model's pretraining parameters
|-- Fine-TuneCheckpoint     # folder that stores model's fine-tuning parameters
|-- SubPrograms         # Extra programming files that clean, extract and split data
|-- .gitignore          # ignore certain file types
|-- Evaluate_LLM.py        # File evaluates models text generation with metrics like BLEU, METEOR and Accuracy
|-- FineTuneDataLoader.py   # File loads and formats data into train and val sets
|-- finetuningtrainloop.py # File exectutes the training loop for finetuning
|-- modelfinetuning.py     # File contains code for the fine-tuning architecture of the transformer model
|-- pretrainmodel.py       # File contains code for the pre-training architecture of the transfrmer model
|-- Pretrainingtrainloop.py   # File exectutes the training loop for pretraining
|-- Prompting.py            # once run, you can prompt the model by submitting your question and answer
|-- README                  # Project Description 
|-- requirements.txt        # Project dependencies 
|-- TestsetLoader.py        # File loads and formats the testing data
|-- tokenizer.json          # contains a dictionary of tokens for each subword etc
|-- utils.py                # additional functions 
```

# Lessons Learned 
This section will focus on highlighting the mistakes that I had made in this project and possible solution to fix problem. The solutions will not be applied as I have moved away from this project. 
But maybe if there is spare time to work on this project again the change will be made:

## Pretraining
Starting off with the pretraining phase the first mistake I notice is that separate word embeddings are being used in both the encoder and decoder, this is both inefficient and impractical as it adds uncessary parameters and the transformer will need to learn new word embeddings from scratch for both the encoder and decoder which, for this specific ask should not be the case, the word embeddings should be between both the encoder and decoder.

Furthermore, upon reviewing how the data was loaded and and batched in to arrays of **x** and **y** there is no real language model being used. Meaning, that there is no coherent task for the model in pretraining. This is because in the **get_batch** function for pretraining, which can be found in **utils.py**, receives a chunk of data and creates a batch_size of indexes that pick a starting position and encompass a block size of input sequence, the batch_size of input sequences are then stacked in a tensor which is x, however y is an attempt at implementing the teacher forcing learning objective by nudging the indexes by +1, however, a start and end token are also concatenated to the input sequence y which are not required for teacher forcing language modelling. Moreover, x, which are batch_size examples of input sequence from the openwebtext dataset, is fed into the encoder and y is fed into the decoder. Once the model produces its logits, the ground truth target that was used in the loss function was y. This was another bug within the pretraining code, because the model is simply copying **y_t** at position **t **because it is being asked to predict the same token It can currently see, which allows the transformer to build an identity mapping which it then will ignore any other components of learning, i.e. x from the cross-attention component. This is definitely one of the reasons why this model produces amazing results in pretraining, but will definitely when prompted produce nonsense or a repetition of the same token.
Therefore in the ReLLM-Marking project I will correctly format a learning objective for pretraining which is the span corruption task used in the T5 model, the link to the paper is here: https://arxiv.org/abs/2401.13160 , this will ensure that the model correctly learns and predicts tokens that it cannot see unlike the setup in the pretraining code.
These were the main standout bugs in the pretraining code, and in the new ‘ReLLM-Marking’ project I will fix these and implement a correctly formatted and working code for the model. 
## Finetuning
Similarly, for finetuning, the way the **y** sequence is formatted is incorrect in both the get_batchfine function, which is found in the **utils.py** file and the **finetunedataloader.py** file. Within the **finetunedataloader.py** the Json file containing the dataset is parsed to obtain the inputs and outputs, the output sequences are the prepended and appended with the start and end token. This step is not needed and can be done in the get_batchfine function. Within the get_batchfine function, the output/y sequences are then batched with no additional changes. However, this means that the y sequence within the model’s architecture will be used as both the input into the decoder and ground truth label to be compared with the logits of the model’s, compute the loss score. This runs into the same bug as the code for pre-training, the model will stop learnng and simple create an identity map to predict next tokens as has to predict the next token in the timestep but that next token is the current token which the model can see, which enables the model to memorize instead of learning during training. 

Therefore in the ReLLLM-Marking project, I will fix this bug, by creating two **y** sequences, I will not concatenate the start and end token to the output sequence in the finetunedataloader.py instead, in get_finebatch, I will have one sequence that is **y** which will contain the output sequence, this sequence will be prepended with a start token. The second sequence will be **y_true**. This will be the ground truth sequence which will be compared to the predictions of the model in the loss function. The **y_true** sequence will be prepended with the end token to make it appear as if it has been shifted one timestep to left making this sequence the correct target for the decoder.

# Conclusion
To conclude, this was my first attempt in building a transformer network, with the end goal of this model being to mark and give feedback to student’s exam answers. While it produced great training results it did perform well in inference at all. While at the time I was puzzled, now looking closely at the source code major bugs have been revealed as to why the model performed well in training but poorly in inference, these bugs are explained in the **Lessons learned** section. To fix these issues, I will attempt to fix all the bugs the code contains and retrain the model, with the hopes that it can now perform well during inference and not only training. This will be done in the project **ReLLM-Marking**, which will be published soon.
